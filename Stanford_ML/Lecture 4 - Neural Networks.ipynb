{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A^T B = (B^TA)^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "$a_j^l$ is the j-th activation node in layer l <br><br>\n",
    "\n",
    "For instance activation node k in layer 2 is calculated as: <br>\n",
    "$a_k^{2} = g(\\sum_{j=0}^J\\theta_{k,j}^1 a_k^{1}) = g(z_k^2)$\n",
    "\n",
    "The hypothesis is equal to the last activation node, e.g in 3 layer NN:\n",
    "\n",
    "$h_{\\theta}(x) = a^3 = g(z_k^3)$ <br>\n",
    "\n",
    "Which means it's actually the same as logistic regression, just with a more complex non-linear feature combination\n",
    "\n",
    "$x_0$ is called the bias variable for each layer and is always equal to 1 !\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "The cost function is actually the same as for logistic regression, just that now we sum over possibly more output nodes or hypothesises. \n",
    "\n",
    "Logistic regression cost functions read like this:\n",
    "\n",
    "$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m} y\\,log(h_{\\theta} x_i) + (1-y)log(1-h_{\\theta} x_i) + \\frac{1}{2m}\\sum_{j=1}^n \\theta_j^2$.$ <br><br>\n",
    "\n",
    "For neural networks it hence is:\n",
    "\n",
    "$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m} \\sum_{k=1}^K \\left[ y_k^i\\,log(h_{\\theta} x_i) + (1-y_k^i)log(1-h_{\\theta} x^i)_k \\right] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1} \\sum_{j=1}^{s_l+1}\\sum_{i=1}^{s_l} (\\theta_{j,i}^l)^2,$ <br><br>\n",
    "\n",
    "where, <br>\n",
    "L = total number of layers <br>\n",
    "$s_l$ = number of units in layer I <br>\n",
    "K = number of output units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Algorithm\n",
    "\n",
    "Intuition: $\\delta_j^l$ is the error of node j in layer l. So the error of node $a_j^l$\n",
    "<br><br>\n",
    "if we have 4 layers, delta_4 (of the last layer ) reads as: <br>\n",
    "$\\delta_j^4 = a_j^4 -y = (h_{\\theta}(x))_j - y $ or in vectorized form: <br>\n",
    "$\\delta^4 = a^4 -y $ <br><br>\n",
    "\n",
    "$\\delta^3 = (\\Theta^3)^T \\delta^{(4)} .* \\, g'(z^{(3)})$<br>\n",
    "$\\delta^2 = (\\Theta^2)^T \\delta^{(3)} .* \\, g'(z^{(2)})$<br><br>\n",
    "\n",
    "Now these rather arbitrarily defined \"Error terms\" begin to makes sense when we show that:<br>\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\Theta_{i,j}^{l}}  J(\\Theta) = a_j^{(l)} \\delta_i^{(l+1)}$ <br><br>\n",
    "\n",
    "Now for n-training examples: <br>\n",
    "Set $\\Delta_{i,j}^{(l)} = 0$, where: <br>\n",
    "i = sample_num <br>\n",
    "j = activation_node_num <br>\n",
    "l = layer <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Theta_{i,j}^{(l)} := \\Theta_{i,j}^{(l)} + a_j^{(l)} \\delta_i^{(l+1)}$ <br>\n",
    "$D_{i,j}^{(l)} := \\frac{1}{m} \\Theta_{i,j}^{(l)} + \\lambda \\Theta_{i,j}^{(l)}$ if j!=0 <br>\n",
    "$D_{i,j}^{(l)} := \\frac{1}{m} \\Theta_{i,j}^{(l)}$ if j==0 <br><br>\n",
    "$\\frac{\\partial}{\\partial \\Theta_{i,j}^{l}}  J(\\Theta) = D_{i,j}^{(l)}$ <br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition\n",
    "We can see $\\delta_k^{(l)}$ as:<br><br>\n",
    "\n",
    "$\\delta_k^{(l)} = \\frac{\\partial}{\\partial z_j^{(l)}} cost(i)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
